{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM on Amazon fine food review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ankit Kumar\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer as sno\n",
    "setofstopwords=set(stopwords.words('english'))\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn= sqlite3.connect('database.sqlite')\n",
    "data= pd.read_sql_query('''\n",
    "SELECT * FROM Reviews WHERE Score!=3\n",
    "''',conn)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing not helpful reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Score']=data['Score'].map(lambda x:'Positive' if x>3 else 'Negative')\n",
    "sorteddata= data.sort_values('ProductId',axis=0)\n",
    "finaldata= sorteddata.drop_duplicates(subset={'UserId','ProfileName',\\\n",
    "        'Time','Text'}, keep='first',inplace=False)\n",
    "\n",
    "finaldata= finaldata[finaldata['HelpfulnessNumerator'] <= finaldata['HelpfulnessDenominator']]\n",
    "data= finaldata.sort_values('Time',axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning HTML, punctuations, apply stemming, lowercasing etc without removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(sentance): #substitute expression contained in <> with ' '\n",
    "    cleaned= re.sub(re.compile('<.*?>'),' ',sentance)\n",
    "    return cleaned\n",
    "#function for removing punctuations chars\n",
    "def cleanpunc(sentance):\n",
    "    cleaned= re.sub(r'[?|!|\\'|\"|#]',r'',sentance)\n",
    "    cleaned= re.sub(r'[.|,|)|(|\\|/]',r'',sentance)\n",
    "    return cleaned\n",
    "snowstem= sno('english')\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "for sent in data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        # we have used cleanpunc(w).split(), one more split function here \n",
    "        # because consider w=\"abc.def\", cleanpunc(w) will return \"abc def\"\n",
    "        # if we dont use .split() function then we will be considring \"abc def\" \n",
    "        # as a single word, but if you use .split() function we will get \"abc\", \"def\"\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                s=(snowstem.stem(cleaned_words.lower())).encode('utf8')\n",
    "                filtered_sentence.append(s)\n",
    "                if(data['Score'].values)[i] =='Positive':\n",
    "                    all_positive_words.append(s)\n",
    "                if(data['Score'].values)[i] =='Negative':\n",
    "                    all_negative_words.append(s)\n",
    "            else:\n",
    "                continue\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    final_string.append(str1)\n",
    "\n",
    "# storing data till now\n",
    "data['CleanedText']=final_string \n",
    "#adding a column of CleanedText which displays the data after pre-processing of the review \n",
    "data['CleanedText']=data['CleanedText'].str.decode(\"utf-8\")\n",
    "    # store final table into an SQlLite table for future.\n",
    "conn = sqlite3.connect('cleanedTextData.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory = str\n",
    "data.to_sql('Reviews', conn,  schema=None, if_exists='replace', \\\n",
    "        index=True, index_label=None, chunksize=None, dtype=None)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>this witti littl book make son laugh loud reci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138683</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>can rememb see the show when air televis year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417839</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>beetlejuic well written movi everyth about fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346055</th>\n",
       "      <td>374359</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "      <td>twist rumplestiskin captur film star michael k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417838</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>beetlejuic excel and funni movi keaton hilari ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId               ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL           shari zychinski   \n",
       "138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "346055  374359  B00004CI84  A344SMIA5JECGM           Vincent P. Ross   \n",
       "417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score       Time  \\\n",
       "138706                     0                       0  Positive  939340800   \n",
       "138683                     2                       2  Positive  940809600   \n",
       "417839                     0                       0  Positive  944092800   \n",
       "346055                     1                       2  Positive  944438400   \n",
       "417838                     0                       0  Positive  946857600   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138706                          EVERY book is educational   \n",
       "138683  This whole series is great way to spend time w...   \n",
       "417839                               Entertainingl Funny!   \n",
       "346055                            A modern day fairy tale   \n",
       "417838                                         FANTASTIC!   \n",
       "\n",
       "                                                     Text  \\\n",
       "138706  this witty little book makes my son laugh at l...   \n",
       "138683  I can remember seeing the show when it aired o...   \n",
       "417839  Beetlejuice is a well written movie ..... ever...   \n",
       "346055  A twist of rumplestiskin captured on film, sta...   \n",
       "417838  Beetlejuice is an excellent and funny movie. K...   \n",
       "\n",
       "                                              CleanedText  \n",
       "138706  this witti littl book make son laugh loud reci...  \n",
       "138683  can rememb see the show when air televis year ...  \n",
       "417839  beetlejuic well written movi everyth about fro...  \n",
       "346055  twist rumplestiskin captur film star michael k...  \n",
       "417838  beetlejuic excel and funni movi keaton hilari ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this confect that has been around few centuri light pillowi citrus gelatin with nut this case filbert and cut into tini squar and then liber coat with powder sugar and tini mouth heaven not too chewi and veri flavor high recommend this yummi treat you are familiar with the stori lion the witch and the this the treat that seduc edmund into sell out his brother and sister the witch'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CleanedText'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking 100k datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data= data[:100000]\n",
    "Data= Data[['CleanedText','Score']]\n",
    "Data['Score']= Data['Score'].map(lambda x:1 if x=='Positive' else 0)\n",
    "\n",
    "Data_x= Data['CleanedText']\n",
    "Data_y= Data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        this witti littl book make son laugh loud reci...\n",
       "1        can rememb see the show when air televis year ...\n",
       "2        beetlejuic well written movi everyth about fro...\n",
       "3        twist rumplestiskin captur film star michael k...\n",
       "4        beetlejuic excel and funni movi keaton hilari ...\n",
       "5        this one movi that should your movi collect fi...\n",
       "6        myself alway enjoy this movi veri funni and en...\n",
       "7        bought few these after apart was infest with f...\n",
       "8        what happen when you say his name three michae...\n",
       "9        get look for beatlejuic french version video r...\n",
       "10       get crazi realli imposs today not find the fre...\n",
       "11       this was realli good idea and the final produc...\n",
       "12       just receiv shipment and could hard wait tri t...\n",
       "13       have just recent purchas the woodstream corp g...\n",
       "14       this are much easier use than the wilson past ...\n",
       "15       these are easi use they not make mess and offe...\n",
       "16       this such great film even know how sum first a...\n",
       "17       beetlejuic wonder amus comed romp that explor ...\n",
       "18       sick scad nasti toothpick all over counter whe...\n",
       "19       thought this movi was funni michael keaton bee...\n",
       "20       mani movi have dealt with the figur death and ...\n",
       "21       know whi anyon would ever use those littl liqu...\n",
       "22       michael keaton bring distinguish characterist ...\n",
       "23       continu amaz the shoddi treatment that some mo...\n",
       "24       just warn you when tri trick you the widescree...\n",
       "25       bought these decor some dia los muerto skull w...\n",
       "26       winona ryder the gothic princess doom see for ...\n",
       "27       this was favorit book mine when was littl girl...\n",
       "28       for year have been tri simul truli italian esp...\n",
       "29       when vacat adam and barbara maitland meet thei...\n",
       "                               ...                        \n",
       "99970    like strong black coffe like tast strong have ...\n",
       "99971    absolut love this tea drink tea everyday and h...\n",
       "99972    about year age our cat would have bladder infe...\n",
       "99973    bought the turkish delight and then read the r...\n",
       "99974    husband recent ate outsid cafe geneva was late...\n",
       "99975    was suffer from cold for almost year spent lot...\n",
       "99976    neighbor and love this candi and difficult fin...\n",
       "99977    dog absolut love yummi chummi use for train tr...\n",
       "99978    this second purchas and plan stay stock with t...\n",
       "99979    was excit when found whole wheat isra couscous...\n",
       "99980    bought dozen these monkey lollipop parti favor...\n",
       "99981    order three set these from entirelypet even ca...\n",
       "99982    special protein plus far favorit cereal the on...\n",
       "99983    was given the opportun review this product bel...\n",
       "99984    gloria jean hit out the park this whi not ther...\n",
       "99985    yuck this coffe tast terribl doe not compet th...\n",
       "99986    order this item becaus when purchas from local...\n",
       "99987    compar other nutrit bar out there find these a...\n",
       "99988    love peanut butter love that they contain ton ...\n",
       "99989    two young adult siberian huski love these they...\n",
       "99990    love teaand realli hate write bad review mayb ...\n",
       "99991    husband and were alway afraid make fish like m...\n",
       "99992    ice breaker ice cube peppermint sugar free gum...\n",
       "99993    say that name this coffe blend appropri found ...\n",
       "99994    was look for coffe replac and this fit the bil...\n",
       "99995    this veri tasti protein shake and give you nic...\n",
       "99996    this the best tast oliv and healthi for you to...\n",
       "99997    sack melitta coffe label fine grind blanc noir...\n",
       "99998    final babi food that tast love that great prot...\n",
       "99999    first ventur salt and happi bought this and sa...\n",
       "Name: CleanedText, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_x.index= [i for i in range(0,10**5)]\n",
    "Data_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Vocabulary set and Frequency dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all words in single list\n",
    "list_= []\n",
    "for i in Data_x:\n",
    "    list_ += i\n",
    "list_= ''.join(list_)\n",
    "allWords=list_.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary= set(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list= list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency dictionary\n",
    "freq_dict= {}\n",
    "for word in vocabulary_list:\n",
    "    freq_dict[word]= allWords.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weightthis': 7,\n",
       " 'againdentist': 1,\n",
       " 'milibar': 1,\n",
       " 'almondopen': 1,\n",
       " 'needuse': 2,\n",
       " 'getcha': 1,\n",
       " 'casealthough': 1,\n",
       " 'excelhusband': 3,\n",
       " 'glycemix': 1,\n",
       " 'pricesuch': 1,\n",
       " 'tastestri': 1,\n",
       " 'enforc': 4,\n",
       " 'adamhave': 1,\n",
       " 'eitherheard': 1,\n",
       " 'gravel': 16,\n",
       " 'fluffihave': 1,\n",
       " 'giftmake': 2,\n",
       " 'futil': 3,\n",
       " 'ratewhen': 1,\n",
       " 'statego': 1,\n",
       " 'dealorvill': 1,\n",
       " 'mcdougalorder': 1,\n",
       " 'art': 123,\n",
       " 'coppicino': 1,\n",
       " 'intensmuch': 1,\n",
       " 'turkeynot': 1,\n",
       " 'prais': 116,\n",
       " 'caseshare': 1,\n",
       " 'lovenormal': 3,\n",
       " 'saltlive': 1,\n",
       " 'railway': 1,\n",
       " 'othercrazi': 1,\n",
       " 'wazu': 1,\n",
       " 'weekcent': 1,\n",
       " 'coloraft': 1,\n",
       " 'asthamt': 1,\n",
       " 'adbig': 1,\n",
       " 'outragtri': 1,\n",
       " 'thougheveryon': 1,\n",
       " 'owenlike': 1,\n",
       " 'stravecchio': 1,\n",
       " 'eatcultiv': 1,\n",
       " 'believthe': 3,\n",
       " 'cartthis': 1,\n",
       " 'issuesat': 1,\n",
       " 'theobromin': 9,\n",
       " 'bussi': 4,\n",
       " 'simiar': 2,\n",
       " 'immediwas': 1,\n",
       " 'promishow': 1,\n",
       " 'detoxifiuse': 1,\n",
       " 'foamread': 1,\n",
       " 'breadmix': 3,\n",
       " 'thesedoctor': 1,\n",
       " 'productcoffe': 1,\n",
       " 'chickenan': 1,\n",
       " 'getorder': 7,\n",
       " 'morecavali': 1,\n",
       " 'youunfroast': 1,\n",
       " 'alikkid': 1,\n",
       " 'totalfirst': 1,\n",
       " 'equalthis': 1,\n",
       " 'carcinogendaughter': 1,\n",
       " 'marketanoth': 1,\n",
       " 'rushbought': 1,\n",
       " 'soot': 2,\n",
       " 'quisp': 9,\n",
       " 'luckbox': 1,\n",
       " 'goeuse': 1,\n",
       " 'kennewicktrust': 1,\n",
       " 'decaffeni': 1,\n",
       " 'stockconveni': 1,\n",
       " 'raenot': 1,\n",
       " 'unspeak': 1,\n",
       " 'andone': 2,\n",
       " 'stratospher': 1,\n",
       " 'pleasedto': 1,\n",
       " 'two': 7369,\n",
       " 'flourand': 4,\n",
       " 'chitosan': 1,\n",
       " 'comparlove': 1,\n",
       " 'gratinstumbl': 1,\n",
       " 'onlilove': 2,\n",
       " 'lolthe': 3,\n",
       " 'dairiuse': 2,\n",
       " 'mooshyso': 1,\n",
       " 'nosenon': 1,\n",
       " 'loofah': 3,\n",
       " 'greatexcel': 3,\n",
       " 'alasken': 1,\n",
       " 'themstop': 1,\n",
       " 'whitebread': 1,\n",
       " 'seedwas': 1,\n",
       " 'menueven': 1,\n",
       " 'flavorokay': 1,\n",
       " 'usualpleasant': 1,\n",
       " 'alternlayla': 1,\n",
       " 'laterveri': 1,\n",
       " 'algaecid': 1,\n",
       " 'revisit': 5,\n",
       " 'flexse': 1,\n",
       " 'muchprobabl': 1,\n",
       " 'closettri': 1,\n",
       " 'differet': 1,\n",
       " 'awhiluse': 1,\n",
       " 'cornnavi': 1,\n",
       " 'helen': 8,\n",
       " 'choc': 103,\n",
       " 'justbasic': 1,\n",
       " 'casegood': 2,\n",
       " 'strawberrifamili': 1,\n",
       " 'spalsh': 1,\n",
       " 'coarsen': 1,\n",
       " 'nondescript': 2,\n",
       " 'mycup': 1,\n",
       " 'waspackag': 1,\n",
       " 'surf': 19,\n",
       " 'guilthave': 2,\n",
       " 'notall': 1,\n",
       " 'clothingbed': 1,\n",
       " 'suretast': 1,\n",
       " 'truliold': 1,\n",
       " 'learnenjoy': 1,\n",
       " 'seizur': 31,\n",
       " 'workother': 1,\n",
       " 'pricehorizon': 1,\n",
       " 'espesci': 1,\n",
       " 'teethyou': 2,\n",
       " 'kidand': 3,\n",
       " 'dealership': 4,\n",
       " 'amaretti': 6,\n",
       " 'carnpuppi': 1,\n",
       " 'açai': 2,\n",
       " 'grip': 20,\n",
       " 'thinkhusband': 1,\n",
       " 'chanakra': 1,\n",
       " 'itbought': 1,\n",
       " 'goooood': 3,\n",
       " 'eitherwas': 2,\n",
       " 'cellophan': 51,\n",
       " 'truththis': 2,\n",
       " 'stape': 1,\n",
       " 'slavethought': 1,\n",
       " 'disgus': 1,\n",
       " 'shotsammi': 1,\n",
       " 'deliciouscrunchyand': 1,\n",
       " 'popcornwith': 1,\n",
       " 'oldthere': 1,\n",
       " 'fromrealli': 2,\n",
       " 'pitathis': 1,\n",
       " 'stickthese': 2,\n",
       " 'youwholesom': 1,\n",
       " 'beathave': 7,\n",
       " 'freshtrident': 1,\n",
       " 'kebabbabi': 1,\n",
       " 'glassput': 1,\n",
       " 'waythese': 15,\n",
       " 'containwent': 1,\n",
       " 'activthis': 2,\n",
       " 'inelig': 2,\n",
       " 'sugaranoth': 1,\n",
       " 'bearalway': 1,\n",
       " 'treatbeen': 5,\n",
       " 'flavorsw': 1,\n",
       " 'dipgot': 1,\n",
       " 'videogam': 1,\n",
       " 'jampreserv': 3,\n",
       " 'tautlove': 1,\n",
       " 'teethgluten': 1,\n",
       " 'andwatch': 1,\n",
       " 'noël': 1,\n",
       " 'tastwhen': 2,\n",
       " 'oventhe': 1,\n",
       " 'horriblewors': 1,\n",
       " 'trirotini': 1,\n",
       " 'urologist': 1,\n",
       " 'presentthis': 5,\n",
       " 'pareil': 3,\n",
       " 'anddeliv': 1,\n",
       " 'etcson': 2,\n",
       " 'model': 81,\n",
       " 'deliverynop': 1,\n",
       " 'coincid': 27,\n",
       " 'ymmvwas': 1,\n",
       " 'recommendwhen': 13,\n",
       " 'zeus': 3,\n",
       " 'purhas': 1,\n",
       " 'formi': 1,\n",
       " 'twiggl': 1,\n",
       " 'mainsteam': 1,\n",
       " 'catechu': 1,\n",
       " 'articifici': 2,\n",
       " 'pekinges': 5,\n",
       " 'balsalm': 4,\n",
       " 'potatowith': 1,\n",
       " 'raini': 37,\n",
       " 'ceylones': 3,\n",
       " 'mesa': 18,\n",
       " 'excelprobabl': 1,\n",
       " 'quantitihonest': 1,\n",
       " 'beefthen': 1,\n",
       " 'sweetentri': 1,\n",
       " 'terrific': 1,\n",
       " 'betterkds': 1,\n",
       " 'caffinate': 1,\n",
       " 'glutensnapea': 1,\n",
       " 'coffebefor': 1,\n",
       " 'erwachsen': 2,\n",
       " 'disloc': 3,\n",
       " 'whatthes': 1,\n",
       " 'elli': 4,\n",
       " 'stufforder': 1,\n",
       " 'greatknow': 1,\n",
       " 'availhad': 8,\n",
       " 'shipwont': 1,\n",
       " 'independ': 53,\n",
       " 'perkinknow': 1,\n",
       " 'controlcreami': 1,\n",
       " 'madebengal': 1,\n",
       " 'nibthe': 1,\n",
       " 'likelihoodonli': 1,\n",
       " 'flavoryear': 1,\n",
       " 'horrif': 18,\n",
       " 'wafersher': 1,\n",
       " 'packagepopcorn': 1,\n",
       " 'proliflove': 1,\n",
       " 'alonlhasa': 1,\n",
       " 'seiyo': 1,\n",
       " 'band': 66,\n",
       " 'neverbean': 1,\n",
       " 'menu': 81,\n",
       " 'growhave': 2,\n",
       " 'restraint': 5,\n",
       " 'huckthought': 1,\n",
       " 'riskthis': 1,\n",
       " 'balsamica': 1,\n",
       " 'amountvolum': 1,\n",
       " 'shouldjack': 1,\n",
       " 'rocklove': 1,\n",
       " 'bight': 1,\n",
       " 'nativwife': 1,\n",
       " 'frozeif': 1,\n",
       " 'milkno': 1,\n",
       " 'dribbl': 15,\n",
       " 'anniv': 3,\n",
       " 'nothwas': 1,\n",
       " 'catorder': 1,\n",
       " 'clamliquid': 1,\n",
       " 'ordertoo': 1,\n",
       " 'productyerbam': 1,\n",
       " 'snackrealli': 4,\n",
       " 'poorlove': 1,\n",
       " 'cabingreat': 1,\n",
       " 'preservrecent': 1,\n",
       " 'gook': 2,\n",
       " 'causfirst': 1,\n",
       " 'joop': 1,\n",
       " 'closegreat': 1,\n",
       " 'shippiong': 1,\n",
       " 'shoppfamili': 1,\n",
       " 'inositol': 6,\n",
       " 'rightth': 4,\n",
       " 'teayummi': 2,\n",
       " 'packagegood': 1,\n",
       " 'lovecould': 4,\n",
       " 'nutritionistthis': 1,\n",
       " 'finead': 1,\n",
       " 'farfall': 3,\n",
       " 'francais': 1,\n",
       " 'otherwhen': 1,\n",
       " 'heavenhave': 2,\n",
       " 'convo': 2,\n",
       " 'youbake': 1,\n",
       " 'hellllooooooooooo': 1,\n",
       " 'reddrink': 1,\n",
       " 'healthierorder': 1,\n",
       " 'antimicrobi': 5,\n",
       " 'difficultth': 1,\n",
       " 'fullactual': 1,\n",
       " 'croy': 1,\n",
       " 'hitchthis': 2,\n",
       " 'daycrunchi': 1,\n",
       " 'lovefat': 1,\n",
       " 'lifeokthey': 1,\n",
       " 'plantthese': 3,\n",
       " 'fix': 546,\n",
       " 'herehey': 1,\n",
       " 'tablet': 239,\n",
       " 'be': 4098,\n",
       " 'lunchstrong': 1,\n",
       " 'trailerfirst': 1,\n",
       " 'enjoyableeveri': 1,\n",
       " 'yourcan': 1,\n",
       " 'patient': 137,\n",
       " 'downswitch': 1,\n",
       " 'juiceit': 2,\n",
       " 'tealove': 31,\n",
       " 'yumolemon': 1,\n",
       " 'driveour': 1,\n",
       " 'fordefinit': 2,\n",
       " 'comelocal': 1,\n",
       " 'fruitveggiegrain': 1,\n",
       " 'drooli': 1,\n",
       " 'ahitunaa': 1,\n",
       " 'flaketook': 1,\n",
       " 'muffaletta': 5,\n",
       " 'bjs': 12,\n",
       " 'wooden': 52,\n",
       " 'separ': 528,\n",
       " 'cookiwonder': 1,\n",
       " 'carbontire': 1,\n",
       " 'flieswith': 1,\n",
       " 'scoobi': 9,\n",
       " 'pleez': 1,\n",
       " 'nagoya': 1,\n",
       " 'trailbraz': 1,\n",
       " 'competituse': 1,\n",
       " 'diceddri': 1,\n",
       " 'quiver': 1,\n",
       " 'changgood': 1,\n",
       " 'regetta': 1,\n",
       " 'sellerall': 1,\n",
       " 'yugoslaviathese': 1,\n",
       " 'smalllove': 2,\n",
       " 'instalthis': 1,\n",
       " 'againgot': 12,\n",
       " 'pricehave': 52,\n",
       " 'fantastanyon': 2,\n",
       " 'szechwan': 2,\n",
       " 'replacwant': 1,\n",
       " 'bedtimcitric': 1,\n",
       " 'happiresearch': 1,\n",
       " 'shipmentyou': 1,\n",
       " 'protienlove': 2,\n",
       " 'embarras': 4,\n",
       " 'teacokewinemilkb': 1,\n",
       " 'yesth': 3,\n",
       " 'etcverithis': 1,\n",
       " 'composthave': 1,\n",
       " 'wound': 85,\n",
       " 'anothtook': 1,\n",
       " 'megaglia': 1,\n",
       " 'storetast': 5,\n",
       " 'splendalbs': 1,\n",
       " 'mate': 207,\n",
       " 'muffindonut': 1,\n",
       " 'bisquwas': 1,\n",
       " 'odorold': 1,\n",
       " 'mixgive': 1,\n",
       " 'truehas': 1,\n",
       " 'teasent': 1,\n",
       " 'math': 76,\n",
       " 'syrupsand': 1,\n",
       " 'welcom': 165,\n",
       " 'tootetley': 1,\n",
       " 'shipalthough': 2,\n",
       " 'preplan': 1,\n",
       " 'reviewgreat': 3,\n",
       " 'shroomlici': 1,\n",
       " 'lessbought': 2,\n",
       " 'thanknot': 1,\n",
       " 'cakeinvest': 1,\n",
       " 'favoritafter': 1,\n",
       " 'richgoat': 1,\n",
       " 'lightish': 1,\n",
       " 'gretchen': 2,\n",
       " 'cavatelli': 2,\n",
       " 'assurtill': 1,\n",
       " 'hannaford': 3,\n",
       " 'microwavdaughter': 1,\n",
       " 'similac': 106,\n",
       " 'coarser': 19,\n",
       " 'strick': 1,\n",
       " 'haveare': 1,\n",
       " 'mealgluten': 1,\n",
       " 'stockabout': 1,\n",
       " 'calisparra': 1,\n",
       " 'andadmit': 1,\n",
       " 'happitook': 1,\n",
       " 'priceyi': 1,\n",
       " 'brita': 6,\n",
       " 'shipabsolut': 1,\n",
       " 'terrificveri': 1,\n",
       " 'teafamili': 6,\n",
       " 'thansk': 1,\n",
       " 'thosegreat': 1,\n",
       " 'foodbar': 2,\n",
       " 'uncurl': 1,\n",
       " 'supplimentnot': 1,\n",
       " 'xylo': 1,\n",
       " 'compair': 10,\n",
       " 'durban': 4,\n",
       " 'shahi': 2,\n",
       " 'suffertri': 1,\n",
       " 'pankosix': 1,\n",
       " 'goopyso': 1,\n",
       " 'measurthis': 1,\n",
       " 'semana': 1,\n",
       " 'msgnow': 1,\n",
       " 'misolove': 1,\n",
       " 'minedeploy': 1,\n",
       " 'cookreceiv': 1,\n",
       " 'mealnice': 1,\n",
       " 'unbrand': 1,\n",
       " 'wolfbe': 1,\n",
       " 'matchaalso': 1,\n",
       " 'fireplac': 13,\n",
       " 'entourag': 1,\n",
       " 'lightboy': 1,\n",
       " 'thingfound': 1,\n",
       " 'jewelosco': 1,\n",
       " 'veriall': 1,\n",
       " 'makerthis': 3,\n",
       " 'packbuy': 3,\n",
       " 'favoritlike': 2,\n",
       " 'packheard': 1,\n",
       " 'thankyou': 15,\n",
       " 'cajun': 74,\n",
       " 'dond': 1,\n",
       " 'groceridog': 1,\n",
       " 'sky': 24,\n",
       " 'packageor': 1,\n",
       " 'wtfyou': 1,\n",
       " 'hiz': 1,\n",
       " 'pincer': 1,\n",
       " 'guiltgood': 1,\n",
       " 'economalreadi': 1,\n",
       " 'nymph': 1,\n",
       " 'recommendlong': 1,\n",
       " 'staph': 6,\n",
       " 'deservnestl': 1,\n",
       " 'sandwhich': 14,\n",
       " 'stockveri': 1,\n",
       " 'sayorder': 1,\n",
       " 'hassl': 141,\n",
       " 'aftertastwhen': 1,\n",
       " 'rangbuy': 1,\n",
       " 'storesomewhat': 1,\n",
       " 'souc': 3,\n",
       " 'tower': 27,\n",
       " 'cupbought': 1,\n",
       " 'sweetnot': 4,\n",
       " 'herepurchas': 2,\n",
       " 'watersh': 1,\n",
       " 'iceblend': 1,\n",
       " 'freezerafter': 1,\n",
       " 'abd': 2,\n",
       " 'excelfinal': 1,\n",
       " 'tastegum': 1,\n",
       " 'cookiyou': 2,\n",
       " 'boccherini': 1,\n",
       " 'mundan': 7,\n",
       " 'toorex': 1,\n",
       " 'customtime': 1,\n",
       " 'kennel': 18,\n",
       " 'itif': 6,\n",
       " 'siteknow': 1,\n",
       " 'wellbeagl': 1,\n",
       " 'wate': 2,\n",
       " 'kestekidi': 4,\n",
       " 'rid': 261,\n",
       " 'grillgood': 2,\n",
       " 'goebelthe': 1,\n",
       " 'sugarlow': 2,\n",
       " 'bitterorder': 1,\n",
       " 'crackerrealli': 1,\n",
       " 'negligexcel': 1,\n",
       " 'flatthese': 1,\n",
       " 'licorich': 1,\n",
       " 'alaea': 3,\n",
       " 'ordereveryon': 1,\n",
       " 'timeorigin': 1,\n",
       " 'exposur': 35,\n",
       " 'swiss': 195,\n",
       " 'againcustom': 1,\n",
       " 'bedsid': 8,\n",
       " 'scotland': 50,\n",
       " 'zincuse': 1,\n",
       " 'piccalilli': 1,\n",
       " 'potao': 2,\n",
       " 'mmmmmmmmmmmmmmmmmmm': 1,\n",
       " 'cheapideal': 1,\n",
       " 'cupcafix': 1,\n",
       " 'thanth': 1,\n",
       " 'flourorgan': 1,\n",
       " 'packaag': 1,\n",
       " 'seaport': 1,\n",
       " 'viger': 2,\n",
       " 'snapthey': 1,\n",
       " 'officfeel': 1,\n",
       " 'rocki': 37,\n",
       " 'wayfriend': 1,\n",
       " 'nostaglia': 1,\n",
       " 'thighship': 1,\n",
       " 'zingsad': 1,\n",
       " 'oilsaltpopcorn': 1,\n",
       " 'brainer': 27,\n",
       " 'tou': 3,\n",
       " 'hostag': 1,\n",
       " 'tastiunfortun': 1,\n",
       " 'currenc': 3,\n",
       " 'where': 3080,\n",
       " 'intriqu': 2,\n",
       " 'weakdrink': 1,\n",
       " 'breaksimpli': 1,\n",
       " 'eatwater': 1,\n",
       " 'whileit': 2,\n",
       " 'sitethey': 1,\n",
       " 'passibl': 1,\n",
       " 'lovegood': 10,\n",
       " 'speghetti': 4,\n",
       " 'ironi': 3,\n",
       " 'notthen': 1,\n",
       " 'oneshame': 1,\n",
       " 'extractlove': 1,\n",
       " 'glutengrain': 1,\n",
       " 'twee': 2,\n",
       " 'foundabsolut': 1,\n",
       " 'jacksboro': 1,\n",
       " 'taiwan': 32,\n",
       " 'pastatasti': 1,\n",
       " 'enoughbrooklynit': 1,\n",
       " 'recommendadmit': 3,\n",
       " 'cape': 23,\n",
       " 'johnhave': 2,\n",
       " 'whilepound': 1,\n",
       " 'pantriwere': 1,\n",
       " 'conditionwhich': 1,\n",
       " 'andwould': 1,\n",
       " 'basiafter': 1,\n",
       " 'syrupthink': 2,\n",
       " 'unrinari': 1,\n",
       " 'thiswell': 5,\n",
       " 'heavithe': 1,\n",
       " 'expel': 28,\n",
       " 'myselfthey': 2,\n",
       " 'rosell': 1,\n",
       " 'guarantebegan': 1,\n",
       " 'coffeshop': 1,\n",
       " 'brandbet': 1,\n",
       " 'eitherreceiv': 1,\n",
       " 'fareit': 1,\n",
       " 'poisen': 5,\n",
       " 'easicould': 1,\n",
       " 'agefor': 1,\n",
       " 'herselfsent': 2,\n",
       " 'scali': 5,\n",
       " 'soeth': 2,\n",
       " 'fungi': 2,\n",
       " 'pursthis': 1,\n",
       " 'ceci': 1,\n",
       " 'cookerlook': 1,\n",
       " 'schultz': 4,\n",
       " 'petit': 166,\n",
       " 'enfamilgrow': 1,\n",
       " 'musicor': 1,\n",
       " 'justrare': 1,\n",
       " 'favoritnormal': 1,\n",
       " 'themlike': 17,\n",
       " 'stockbeen': 1,\n",
       " 'themhave': 86,\n",
       " 'thinggrape': 1,\n",
       " 'deali': 2,\n",
       " 'nightnever': 1,\n",
       " 'buzzlove': 1,\n",
       " 'dailiexcel': 1,\n",
       " 'eggplantnot': 1,\n",
       " 'tabletson': 1,\n",
       " 'itallian': 2,\n",
       " 'chocki': 2,\n",
       " 'serimonth': 1,\n",
       " 'leastregular': 1,\n",
       " 'toomin': 1,\n",
       " 'alumnus': 1,\n",
       " 'howbuy': 1,\n",
       " 'evengrew': 1,\n",
       " 'bob': 49,\n",
       " 'stockcat': 2,\n",
       " 'hereknow': 1,\n",
       " 'pomegranhave': 1,\n",
       " 'foundrica': 1,\n",
       " 'starboy': 1,\n",
       " 'likeamazon': 1,\n",
       " 'zum': 1,\n",
       " 'whatevwild': 1,\n",
       " 'cantthese': 1,\n",
       " 'packagproduct': 1,\n",
       " 'appetitpurchas': 2,\n",
       " 'loooov': 14,\n",
       " 'daythere': 1,\n",
       " 'rightcustom': 1,\n",
       " 'raeli': 1,\n",
       " 'awarthis': 1,\n",
       " 'mamma': 5,\n",
       " 'loner': 1,\n",
       " 'spicemouth': 1,\n",
       " 'producreceiv': 1,\n",
       " 'nutrientlove': 1,\n",
       " 'krispihas': 1,\n",
       " 'ointment': 5,\n",
       " 'liyaqat': 1,\n",
       " 'thanxwent': 1,\n",
       " 'irishdelici': 1,\n",
       " 'sharp': 276,\n",
       " 'veriour': 2,\n",
       " 'yucko': 1,\n",
       " 'eatergrew': 1,\n",
       " 'choicbeen': 2,\n",
       " 'coffewould': 3,\n",
       " 'busi': 971,\n",
       " 'schoolsorri': 1,\n",
       " 'forador': 2,\n",
       " 'dilemmathis': 1,\n",
       " 'davisonnew': 1,\n",
       " 'molecul': 37,\n",
       " 'attributwhen': 1,\n",
       " 'homini': 5,\n",
       " 'toooo': 7,\n",
       " 'caserecal': 1,\n",
       " 'staplfirst': 1,\n",
       " 'foutain': 1,\n",
       " 'menus': 10,\n",
       " 'closewhen': 1,\n",
       " 'washno': 1,\n",
       " 'bestgive': 2,\n",
       " 'aromaeasi': 1,\n",
       " 'trapknow': 1,\n",
       " 'tek': 5,\n",
       " 'saymotor': 1,\n",
       " 'gutter': 4,\n",
       " 'chattanooga': 2,\n",
       " 'itdo': 2,\n",
       " 'waterbelt': 1,\n",
       " 'hibachi': 2,\n",
       " 'varihad': 1,\n",
       " 'discriminatori': 1,\n",
       " 'wet': 495,\n",
       " 'overknead': 1,\n",
       " 'readthis': 1,\n",
       " 'bikefirst': 1,\n",
       " 'upright': 27,\n",
       " 'wormnecco': 1,\n",
       " 'yourramun': 1,\n",
       " 'coupl': 2404,\n",
       " 'charit': 15,\n",
       " 'thankssgave': 1,\n",
       " 'thenquit': 1,\n",
       " 'bouight': 1,\n",
       " 'tag': 136,\n",
       " 'againonc': 3,\n",
       " 'vitamineveryon': 1,\n",
       " 'listsighsshockthese': 1,\n",
       " 'dark': 3020,\n",
       " 'toysrus': 1,\n",
       " 'fatteningso': 1,\n",
       " 'internetorder': 1,\n",
       " 'blant': 1,\n",
       " 'thhey': 1,\n",
       " 'beatthre': 1,\n",
       " 'herefound': 2,\n",
       " 'munchkinread': 1,\n",
       " 'teaagre': 1,\n",
       " 'orderive': 1,\n",
       " 'timeferment': 1,\n",
       " 'alonhave': 3,\n",
       " 'mealit': 3,\n",
       " 'metabisulfit': 6,\n",
       " 'afternoonmichael': 1,\n",
       " 'yogurtkefir': 1,\n",
       " 'hindenberg': 1,\n",
       " 'granbabi': 1,\n",
       " 'teaingredi': 1,\n",
       " 'hardi': 34,\n",
       " 'likehome': 1,\n",
       " 'marintri': 1,\n",
       " 'buildwhen': 1,\n",
       " 'lookclassic': 1,\n",
       " 'ricola': 5,\n",
       " 'plusthese': 2,\n",
       " 'cheesbest': 1,\n",
       " 'tierra': 8,\n",
       " 'hayguiltless': 1,\n",
       " 'exam': 21,\n",
       " 'mannercannot': 1,\n",
       " 'diffeent': 1,\n",
       " 'treatwow': 1,\n",
       " 'addbought': 2,\n",
       " 'absolutey': 1,\n",
       " 'lean': 205,\n",
       " 'theseediet': 1,\n",
       " 'drinksent': 1,\n",
       " 'mento': 2,\n",
       " 'regress': 2,\n",
       " 'twoand': 1,\n",
       " 'stora': 1,\n",
       " 'warmmayb': 1,\n",
       " 'tjs': 4,\n",
       " 'theoften': 1,\n",
       " 'specialgreat': 1,\n",
       " 'smoothivisit': 1,\n",
       " 'lookneed': 1,\n",
       " 'occasthorough': 1,\n",
       " 'heartilihave': 1,\n",
       " 'freshlik': 1,\n",
       " 'pst': 1,\n",
       " 'gettingin': 1,\n",
       " 'labelfirst': 1,\n",
       " 'thse': 3,\n",
       " 'nevella': 1,\n",
       " 'potatoover': 1,\n",
       " 'teahad': 6,\n",
       " 'catthis': 10,\n",
       " 'interestnot': 1,\n",
       " 'productveryy': 1,\n",
       " 'lessd': 1,\n",
       " 'cheapso': 2,\n",
       " 'sunrischild': 1,\n",
       " 'bothertrident': 1,\n",
       " 'triglyceridbought': 1,\n",
       " 'sugarkid': 1,\n",
       " 'horriblthis': 2,\n",
       " 'inulabrador': 1,\n",
       " 'holidaythis': 8,\n",
       " 'bitbuy': 1,\n",
       " 'calorigotten': 1,\n",
       " 'spotlet': 1,\n",
       " 'studentthere': 1,\n",
       " 'flavourbought': 1,\n",
       " 'othersorri': 1,\n",
       " 'lotsalt': 1,\n",
       " 'bluemountain': 1,\n",
       " 'disappointedthi': 1,\n",
       " 'alreadirealli': 1,\n",
       " 'dick': 24,\n",
       " 'beanwell': 1,\n",
       " 'rumplestiskin': 1,\n",
       " 'dayultim': 1,\n",
       " 'texturspice': 1,\n",
       " 'pacehave': 1,\n",
       " 'anipurchas': 2,\n",
       " 'aris': 11,\n",
       " 'flatulencega': 1,\n",
       " 'withgrate': 1,\n",
       " 'likesit': 1,\n",
       " 'outthose': 1,\n",
       " 'alternthought': 1,\n",
       " 'moldhave': 1,\n",
       " 'nightwhen': 2,\n",
       " 'formade': 2,\n",
       " 'happenednev': 1,\n",
       " 'healthirealli': 1,\n",
       " 'situatdark': 1,\n",
       " 'lostthey': 1,\n",
       " 'blazekbulli': 1,\n",
       " 'moreset': 1,\n",
       " 'recommendani': 2,\n",
       " 'eventwo': 1,\n",
       " 'reasontri': 2,\n",
       " 'contentjust': 1,\n",
       " 'madeenah': 1,\n",
       " 'mixfolger': 1,\n",
       " 'tukey': 1,\n",
       " 'watercombin': 1,\n",
       " 'outerwear': 1,\n",
       " 'againi': 20,\n",
       " 'donepleas': 1,\n",
       " 'effectarrowhead': 1,\n",
       " 'companiwell': 1,\n",
       " 'beatjust': 1,\n",
       " 'decisbeen': 1,\n",
       " 'craver': 1,\n",
       " 'herbalearthi': 1,\n",
       " 'collielab': 2,\n",
       " 'traveleight': 1,\n",
       " 'mixturewhich': 1,\n",
       " 'anymor': 805,\n",
       " 'specialeveri': 1,\n",
       " 'amazoncomdog': 1,\n",
       " 'hunt': 181,\n",
       " 'toghterit': 1,\n",
       " 'bellino': 1,\n",
       " 'uniqueif': 1,\n",
       " 'coursclear': 1,\n",
       " 'elswell': 1,\n",
       " 'amazonfriend': 2,\n",
       " 'moisturlive': 1,\n",
       " 'lessdisappoint': 1,\n",
       " 'nexium': 4,\n",
       " 'carryng': 1,\n",
       " 'chartreus': 8,\n",
       " 'handnever': 2,\n",
       " 'teir': 1,\n",
       " 'foodfightgroceri': 1,\n",
       " 'asda': 1,\n",
       " 'nubbut': 1,\n",
       " 'rivalsanyon': 1,\n",
       " 'couton': 1,\n",
       " 'blendyourselv': 1,\n",
       " 'winebase': 1,\n",
       " 'cleardri': 1,\n",
       " 'wubba': 1,\n",
       " 'itzaman': 1,\n",
       " 'peachmango': 3,\n",
       " 'controlyou': 1,\n",
       " 'satisfisome': 1,\n",
       " 'pickiyou': 1,\n",
       " 'harrar': 3,\n",
       " 'chilliyet': 1,\n",
       " 'dogthe': 9,\n",
       " 'stalest': 1,\n",
       " 'detoxif': 3,\n",
       " 'schnere': 2,\n",
       " 'mandatori': 8,\n",
       " 'alwayrecent': 1,\n",
       " 'ozbottl': 1,\n",
       " 'teaour': 2,\n",
       " 'andoill': 1,\n",
       " 'realthank': 1,\n",
       " 'continthis': 1,\n",
       " 'loacker': 1,\n",
       " 'onecare': 1,\n",
       " 'personyou': 1,\n",
       " 'fraenkisch': 1,\n",
       " 'thebegin': 1,\n",
       " 'octopus': 29,\n",
       " 'mileafter': 1,\n",
       " 'candithank': 1,\n",
       " 'sobat': 1,\n",
       " 'sacrifici': 1,\n",
       " 'messfinal': 1,\n",
       " 'hugest': 3,\n",
       " 'shipmentit': 2,\n",
       " 'expectare': 1,\n",
       " 'surfacwonder': 1,\n",
       " 'availchees': 1,\n",
       " 'getwhi': 1,\n",
       " 'wellahmad': 1,\n",
       " 'anythlove': 9,\n",
       " 'applebutt': 1,\n",
       " 'thisown': 2,\n",
       " 'saucett': 5,\n",
       " 'themfar': 3,\n",
       " 'huski': 38,\n",
       " 'miscommun': 1,\n",
       " 'superstit': 1,\n",
       " 'availablehmmhidmade': 1,\n",
       " 'picturgreat': 1,\n",
       " 'freshbare': 1,\n",
       " 'tastewil': 3,\n",
       " 'bon': 31,\n",
       " 'cajol': 6,\n",
       " 'cokki': 1,\n",
       " 'cupit': 2,\n",
       " 'futurafter': 2,\n",
       " 'themithese': 1,\n",
       " 'theirfirst': 2,\n",
       " 'usaand': 1,\n",
       " 'aupperltri': 1,\n",
       " 'frichikagre': 1,\n",
       " 'thoughtri': 6,\n",
       " 'floursmelltast': 1,\n",
       " 'tablhard': 1,\n",
       " 'setfast': 1,\n",
       " 'stapltwo': 1,\n",
       " 'expectdog': 2,\n",
       " 'gummithe': 1,\n",
       " 'monthwhen': 2,\n",
       " 'deliciamerican': 1,\n",
       " 'thatthought': 2,\n",
       " 'brandsecond': 1,\n",
       " 'daffodil': 1,\n",
       " 'schooler': 1,\n",
       " 'groceriuse': 2,\n",
       " 'epa': 2,\n",
       " 'sandwichdid': 1,\n",
       " 'drainmaxwel': 1,\n",
       " 'waikato': 2,\n",
       " 'sryup': 1,\n",
       " 'aromatthese': 1,\n",
       " 'crowdtazo': 1,\n",
       " 'competitthis': 4,\n",
       " 'bakenoth': 1,\n",
       " 'proteinlove': 2,\n",
       " 'goldbox': 7,\n",
       " 'minerwas': 1,\n",
       " 'mile': 279,\n",
       " 'butterthank': 1,\n",
       " 'originthis': 2,\n",
       " 'portionwas': 1,\n",
       " 'insectthis': 1,\n",
       " 'forign': 1,\n",
       " 'themay': 3,\n",
       " 'recievthis': 2,\n",
       " 'duperealli': 1,\n",
       " 'simon': 3,\n",
       " 'recommendhope': 1,\n",
       " 'scant': 23,\n",
       " 'counteract': 19,\n",
       " 'excelhave': 12,\n",
       " 'monthrealli': 3,\n",
       " 'thankdecid': 2,\n",
       " 'disillusion': 1,\n",
       " 'indig': 2,\n",
       " 'availwas': 1,\n",
       " 'rydermich': 1,\n",
       " 'smoother': 155,\n",
       " 'rotessir': 1,\n",
       " 'nutrientsnot': 1,\n",
       " 'thoughwellthi': 1,\n",
       " 'theuse': 16,\n",
       " 'exceedanoth': 1,\n",
       " 'foodtwo': 1,\n",
       " 'teawhen': 5,\n",
       " 'instantyou': 1,\n",
       " 'grandeur': 1,\n",
       " 'packfine': 1,\n",
       " 'altinba': 1,\n",
       " 'breakfastfound': 2,\n",
       " 'theirour': 1,\n",
       " 'anymorrealli': 4,\n",
       " 'noodljust': 1,\n",
       " 'potaot': 1,\n",
       " 'lifethese': 6,\n",
       " 'printmi': 1,\n",
       " 'earlier': 217,\n",
       " 'muniecthis': 1,\n",
       " 'storewow': 1,\n",
       " 'enjoyexpect': 1,\n",
       " 'localmccormick': 1,\n",
       " 'benzodiazepin': 2,\n",
       " 'programthese': 2,\n",
       " 'weakest': 12,\n",
       " 'unecessaripuppi': 1,\n",
       " 'confectionari': 12,\n",
       " 'goneknow': 2,\n",
       " 'easpurchas': 1,\n",
       " 'shotthis': 3,\n",
       " 'tenia': 1,\n",
       " 'saucproduct': 2,\n",
       " 'sneek': 2,\n",
       " 'bulkour': 1,\n",
       " 'debilitatingi': 1,\n",
       " 'sweetenwow': 1,\n",
       " 'moneyfamili': 1,\n",
       " 'barsbut': 1,\n",
       " 'kos': 1,\n",
       " 'nicesuper': 1,\n",
       " 'poundhusband': 1,\n",
       " 'mustwas': 1,\n",
       " 'greec': 33,\n",
       " 'availproduct': 1,\n",
       " 'beat': 1067,\n",
       " 'tastfirst': 12,\n",
       " 'thisfinicki': 2,\n",
       " 'costpup': 1,\n",
       " 'alluse': 8,\n",
       " 'yougrown': 1,\n",
       " 'bigot': 1,\n",
       " 'shatilause': 1,\n",
       " 'repack': 1,\n",
       " 'marihave': 1,\n",
       " 'perdaughter': 1,\n",
       " 'triwonder': 3,\n",
       " 'coffeewithout': 1,\n",
       " 'bodyand': 1,\n",
       " 'experipath': 1,\n",
       " 'muahahaha': 1,\n",
       " 'fromveget': 1,\n",
       " 'farmwas': 1,\n",
       " 'pickey': 4,\n",
       " 'toocant': 1,\n",
       " 'theheard': 1,\n",
       " 'swellew': 1,\n",
       " 'grainluv': 1,\n",
       " 'marketgave': 1,\n",
       " 'paythese': 4,\n",
       " 'farro': 14,\n",
       " 'sticksbranch': 1,\n",
       " 'diffenc': 2,\n",
       " 'costlike': 1,\n",
       " 'afternoonread': 1,\n",
       " 'vinegarwhat': 1,\n",
       " 'chuppa': 1,\n",
       " 'enjoyyear': 3,\n",
       " 'sellerbunnibig': 1,\n",
       " 'abbott': 3,\n",
       " 'themalmond': 1,\n",
       " 'zabuca': 1,\n",
       " 'baradd': 1,\n",
       " 'usetoo': 2,\n",
       " 'storealthough': 2,\n",
       " 'jerkithese': 5,\n",
       " 'herecreami': 1,\n",
       " 'dairymen': 1,\n",
       " 'oatblueberri': 1,\n",
       " 'honeyorder': 1,\n",
       " 'jacket': 28,\n",
       " 'impulsethey': 1,\n",
       " 'cranberriswitch': 1,\n",
       " 'binghamton': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('freq_dict.pkl','wb') as file:\n",
    "    pickle.dump(freq_dict,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating rank list of frequent words upto 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted_list= []\n",
    "for k, v in sorted(freq_dict.items(), key=itemgetter(1),reverse=True):\n",
    "    sorted_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'this',\n",
       " 'for',\n",
       " 'that',\n",
       " 'with',\n",
       " 'you',\n",
       " 'have',\n",
       " 'but',\n",
       " 'are',\n",
       " 'not',\n",
       " 'they',\n",
       " 'was',\n",
       " 'like',\n",
       " 'tast',\n",
       " 'flavor',\n",
       " 'them',\n",
       " 'these',\n",
       " 'good',\n",
       " 'tea',\n",
       " 'one',\n",
       " 'use',\n",
       " 'can',\n",
       " 'product',\n",
       " 'veri',\n",
       " 'great',\n",
       " 'just',\n",
       " 'tri',\n",
       " 'all',\n",
       " 'from',\n",
       " 'love',\n",
       " 'make',\n",
       " 'has',\n",
       " 'when',\n",
       " 'get',\n",
       " 'more',\n",
       " 'other',\n",
       " 'will',\n",
       " 'than',\n",
       " 'coffe',\n",
       " 'had',\n",
       " 'out',\n",
       " 'would',\n",
       " 'some',\n",
       " 'buy',\n",
       " 'food',\n",
       " 'onli',\n",
       " 'eat',\n",
       " 'about',\n",
       " 'time',\n",
       " 'your',\n",
       " 'find',\n",
       " 'realli',\n",
       " 'also',\n",
       " 'best',\n",
       " 'much',\n",
       " 'too',\n",
       " 'littl',\n",
       " 'order',\n",
       " 'even',\n",
       " 'amazon',\n",
       " 'becaus',\n",
       " 'drink',\n",
       " 'which',\n",
       " 'were',\n",
       " 'price',\n",
       " 'bag',\n",
       " 'there',\n",
       " 'store',\n",
       " 'been',\n",
       " 'mix',\n",
       " 'what',\n",
       " 'chocol',\n",
       " 'ani',\n",
       " 'better',\n",
       " 'well',\n",
       " 'box',\n",
       " 'sugar',\n",
       " 'now',\n",
       " 'year',\n",
       " 'their',\n",
       " 'after',\n",
       " 'sweet',\n",
       " 'found',\n",
       " 'day',\n",
       " 'dog',\n",
       " 'want',\n",
       " 'then',\n",
       " 'high',\n",
       " 'look',\n",
       " 'our',\n",
       " 'give',\n",
       " 'cup',\n",
       " 'over',\n",
       " 'first',\n",
       " 'add',\n",
       " 'water',\n",
       " 'brand',\n",
       " 'recommend',\n",
       " 'most',\n",
       " 'she',\n",
       " 'made',\n",
       " 'think',\n",
       " 'packag',\n",
       " 'way',\n",
       " 'who',\n",
       " 'treat',\n",
       " 'two',\n",
       " 'nice',\n",
       " 'work',\n",
       " 'mani',\n",
       " 'enjoy',\n",
       " 'sinc',\n",
       " 'favorit',\n",
       " 'need',\n",
       " 'thing',\n",
       " 'know',\n",
       " 'bar',\n",
       " 'keep',\n",
       " 'bit',\n",
       " 'come',\n",
       " 'differ',\n",
       " 'milk',\n",
       " 'could',\n",
       " 'purchas',\n",
       " 'say',\n",
       " 'snack',\n",
       " 'still',\n",
       " 'lot',\n",
       " 'free',\n",
       " 'delici',\n",
       " 'pack',\n",
       " 'ship',\n",
       " 'hot',\n",
       " 'her',\n",
       " 'take',\n",
       " 'never',\n",
       " 'review',\n",
       " 'organ',\n",
       " 'into',\n",
       " 'without',\n",
       " 'perfect',\n",
       " 'wonder',\n",
       " 'fresh',\n",
       " 'everi',\n",
       " 'doe',\n",
       " 'ever',\n",
       " 'befor',\n",
       " 'how',\n",
       " 'ingredi',\n",
       " 'local',\n",
       " 'sauc',\n",
       " 'cook',\n",
       " 'cat',\n",
       " 'few',\n",
       " 'alway',\n",
       " 'easi',\n",
       " 'bought',\n",
       " 'put',\n",
       " 'natur',\n",
       " 'someth',\n",
       " 'stuff',\n",
       " 'seem',\n",
       " 'cooki',\n",
       " 'it',\n",
       " 'oil',\n",
       " 'whole',\n",
       " 'healthi',\n",
       " 'green',\n",
       " 'contain',\n",
       " 'did',\n",
       " 'got',\n",
       " 'enough',\n",
       " 'hard',\n",
       " 'while',\n",
       " 'ad',\n",
       " 'right',\n",
       " 'qualiti',\n",
       " 'rice',\n",
       " 'those',\n",
       " 'same',\n",
       " 'back',\n",
       " 'regular',\n",
       " 'less',\n",
       " 'dri',\n",
       " 'last',\n",
       " 'candi',\n",
       " 'small',\n",
       " 'salt',\n",
       " 'cereal',\n",
       " 'here',\n",
       " 'calori',\n",
       " 'again',\n",
       " 'howev',\n",
       " 'long',\n",
       " 'serv',\n",
       " 'fruit',\n",
       " 'groceri',\n",
       " 'each',\n",
       " 'actual',\n",
       " 'size',\n",
       " 'tasti',\n",
       " 'quick',\n",
       " 'quit',\n",
       " 'feel',\n",
       " 'see',\n",
       " 'far',\n",
       " 'sure',\n",
       " 'old',\n",
       " 'strong',\n",
       " 'excel',\n",
       " 'definit',\n",
       " 'off',\n",
       " 'though',\n",
       " 'both',\n",
       " 'month',\n",
       " 'textur',\n",
       " 'his',\n",
       " 'peopl',\n",
       " 'chip',\n",
       " 'bread',\n",
       " 'juic',\n",
       " 'bottl',\n",
       " 'be',\n",
       " 'help',\n",
       " 'chicken',\n",
       " 'bean',\n",
       " 'problem',\n",
       " 'start',\n",
       " 'anoth',\n",
       " 'big',\n",
       " 'real',\n",
       " 'through',\n",
       " 'down',\n",
       " 'fat',\n",
       " 'smell',\n",
       " 'item',\n",
       " 'butter',\n",
       " 'open',\n",
       " 'bad',\n",
       " 'case',\n",
       " 'soup',\n",
       " 'almost',\n",
       " 'famili',\n",
       " 'blend',\n",
       " 'kid',\n",
       " 'chees',\n",
       " 'may',\n",
       " 'pretti',\n",
       " 'sever',\n",
       " 'happi',\n",
       " 'usual',\n",
       " 'per',\n",
       " 'thought',\n",
       " 'friend',\n",
       " 'low',\n",
       " 'should',\n",
       " 'go',\n",
       " 'top',\n",
       " 'black',\n",
       " 'new',\n",
       " 'bake',\n",
       " 'gluten',\n",
       " 'diet',\n",
       " 'amount',\n",
       " 'anyth',\n",
       " 'avail',\n",
       " 'varieti',\n",
       " 'compani',\n",
       " 'pasta',\n",
       " 'thank',\n",
       " 'worth',\n",
       " 'light',\n",
       " 'recip',\n",
       " 'minut',\n",
       " 'peanut',\n",
       " 'own',\n",
       " 'around',\n",
       " 'expens',\n",
       " 'onc',\n",
       " 'arriv',\n",
       " 'ice',\n",
       " 'kind',\n",
       " 'nut',\n",
       " 'home',\n",
       " 'chew',\n",
       " 'prefer',\n",
       " 'reason',\n",
       " 'full',\n",
       " 'protein',\n",
       " 'where',\n",
       " 'week',\n",
       " 'health',\n",
       " 'morn',\n",
       " 'half',\n",
       " 'especi',\n",
       " 'abl',\n",
       " 'came',\n",
       " 'dark',\n",
       " 'syrup',\n",
       " 'meal',\n",
       " 'carri',\n",
       " 'receiv',\n",
       " 'until',\n",
       " 'powder',\n",
       " 'probabl',\n",
       " 'white',\n",
       " 'spice',\n",
       " 'save',\n",
       " 'cost',\n",
       " 'expect',\n",
       " 'bitter',\n",
       " 'gift',\n",
       " 'him',\n",
       " 'leav',\n",
       " 'fill',\n",
       " 'piec',\n",
       " 'brew',\n",
       " 'pleas',\n",
       " 'honey',\n",
       " 'might',\n",
       " 'said',\n",
       " 'breakfast',\n",
       " 'such',\n",
       " 'hand',\n",
       " 'roast',\n",
       " 'star',\n",
       " 'away',\n",
       " 'vanilla',\n",
       " 'corn',\n",
       " 'call',\n",
       " 'fact',\n",
       " 'rich',\n",
       " 'place',\n",
       " 'larg',\n",
       " 'compar',\n",
       " 'extra',\n",
       " 'absolut',\n",
       " 'cream',\n",
       " 'three',\n",
       " 'instead',\n",
       " 'live',\n",
       " 'whi',\n",
       " 'coconut',\n",
       " 'sweeten',\n",
       " 'hope',\n",
       " 'cake',\n",
       " 'noth',\n",
       " 'read',\n",
       " 'wheat',\n",
       " 'type',\n",
       " 'disappoint',\n",
       " 'red',\n",
       " 'surpris',\n",
       " 'least',\n",
       " 'let',\n",
       " 'season',\n",
       " 'coupl',\n",
       " 'market',\n",
       " 'cracker',\n",
       " 'slight',\n",
       " 'wish',\n",
       " 'care',\n",
       " 'satisfi',\n",
       " 'smooth',\n",
       " 'ago',\n",
       " 'fine',\n",
       " 'decid',\n",
       " 'husband',\n",
       " 'meat',\n",
       " 'jar',\n",
       " 'end',\n",
       " 'turn',\n",
       " 'soda',\n",
       " 'second',\n",
       " 'addit',\n",
       " 'yet',\n",
       " 'next',\n",
       " 'deal',\n",
       " 'longer',\n",
       " 'pepper',\n",
       " 'chang',\n",
       " 'plus',\n",
       " 'stop',\n",
       " 'although',\n",
       " 'offer',\n",
       " 'run',\n",
       " 'includ',\n",
       " 'color',\n",
       " 'tell',\n",
       " 'went',\n",
       " 'list',\n",
       " 'must',\n",
       " 'sell',\n",
       " 'anyon',\n",
       " 'money',\n",
       " 'either',\n",
       " 'person',\n",
       " 'packet',\n",
       " 'cherri',\n",
       " 'spici',\n",
       " 'altern',\n",
       " 'fiber',\n",
       " 'noodl',\n",
       " 'orang',\n",
       " 'dish',\n",
       " 'gave',\n",
       " 'almond',\n",
       " 'glad',\n",
       " 'mayb',\n",
       " 'salad',\n",
       " 'side',\n",
       " 'appl',\n",
       " 'oliv',\n",
       " 'part',\n",
       " 'hous',\n",
       " 'sometim',\n",
       " 'amaz',\n",
       " 'stick',\n",
       " 'notic',\n",
       " 'etc',\n",
       " 'fast',\n",
       " 'soft',\n",
       " 'version',\n",
       " 'heat',\n",
       " 'seed',\n",
       " 'cheaper',\n",
       " 'past',\n",
       " 'soy',\n",
       " 'cold',\n",
       " 'everyth',\n",
       " 'origin',\n",
       " 'believ',\n",
       " 'fan',\n",
       " 'everyon',\n",
       " 'cut',\n",
       " 'els',\n",
       " 'rather',\n",
       " 'mouth',\n",
       " 'pay',\n",
       " 'pod',\n",
       " 'choic',\n",
       " 'myself',\n",
       " 'crunchi',\n",
       " 'flour',\n",
       " 'stock',\n",
       " 'hour',\n",
       " 'conveni',\n",
       " 'brown',\n",
       " 'oatmeal',\n",
       " 'experi',\n",
       " 'took',\n",
       " 'tomato',\n",
       " 'gum',\n",
       " 'potato',\n",
       " 'bowl',\n",
       " 'weight',\n",
       " 'ounc',\n",
       " 'special',\n",
       " 'lemon',\n",
       " 'close',\n",
       " 'often',\n",
       " 'popcorn',\n",
       " 'son',\n",
       " 'nutrit',\n",
       " 'direct',\n",
       " 'espresso',\n",
       " 'cinnamon',\n",
       " 'exact',\n",
       " 'total',\n",
       " 'energi',\n",
       " 'egg',\n",
       " 'clean',\n",
       " 'easili',\n",
       " 'pound',\n",
       " 'valu',\n",
       " 'grain',\n",
       " 'ask',\n",
       " 'prepar',\n",
       " 'mean',\n",
       " 'consist',\n",
       " 'granola',\n",
       " 'plain',\n",
       " 'goe',\n",
       " 'onlin',\n",
       " 'base',\n",
       " 'normal',\n",
       " 'feed',\n",
       " 'strawberri',\n",
       " 'switch',\n",
       " 'artifici',\n",
       " 'set',\n",
       " 'machin',\n",
       " 'result',\n",
       " 'vitamin',\n",
       " 'caffein',\n",
       " 'life',\n",
       " 'mild',\n",
       " 'becom',\n",
       " 'combin',\n",
       " 'near',\n",
       " 'pot',\n",
       " 'recent',\n",
       " 'particular',\n",
       " 'cours',\n",
       " 'bite',\n",
       " 'microwav',\n",
       " 'simpli',\n",
       " 'pop',\n",
       " 'cannot',\n",
       " 'stay',\n",
       " 'final',\n",
       " 'label',\n",
       " 'bodi',\n",
       " 'instant',\n",
       " 'complet',\n",
       " 'replac',\n",
       " 'carb',\n",
       " 'beef',\n",
       " 'date',\n",
       " 'ground',\n",
       " 'salti',\n",
       " 'similar',\n",
       " 'wait',\n",
       " 'certain',\n",
       " 'sodium',\n",
       " 'dure',\n",
       " 'babi',\n",
       " 'process',\n",
       " 'night',\n",
       " 'search',\n",
       " 'teeth',\n",
       " 'substitut',\n",
       " 'between',\n",
       " 'plastic',\n",
       " 'pleasant',\n",
       " 'lunch',\n",
       " 'suggest',\n",
       " 'aroma',\n",
       " 'ginger',\n",
       " 'consid',\n",
       " 'continu',\n",
       " 'shop',\n",
       " 'line',\n",
       " 'bring',\n",
       " 'cocoa',\n",
       " 'super',\n",
       " 'effect',\n",
       " 'benefit',\n",
       " 'name',\n",
       " 'check',\n",
       " 'content',\n",
       " 'fair',\n",
       " 'pick',\n",
       " 'four',\n",
       " 'daughter',\n",
       " 'veget',\n",
       " 'except',\n",
       " 'servic',\n",
       " 'chewi',\n",
       " 'bulk',\n",
       " 'gram',\n",
       " 'finish',\n",
       " 'deliv',\n",
       " 'smaller',\n",
       " 'pancak',\n",
       " 'chai',\n",
       " 'extrem',\n",
       " 'rate',\n",
       " 'oat',\n",
       " 'daili',\n",
       " 'along',\n",
       " 'individu',\n",
       " 'pure',\n",
       " 'addict',\n",
       " 'guess',\n",
       " 'rememb',\n",
       " 'state',\n",
       " 'fish',\n",
       " 'left',\n",
       " 'point',\n",
       " 'eaten',\n",
       " 'provid',\n",
       " 'sit',\n",
       " 'yummi',\n",
       " 'pill',\n",
       " 'berri',\n",
       " 'christma',\n",
       " 'note',\n",
       " 'delight',\n",
       " 'entir',\n",
       " 'overal',\n",
       " 'given',\n",
       " 'formula',\n",
       " 'decaf',\n",
       " 'custom',\n",
       " 'singl',\n",
       " 'gummi',\n",
       " 'itself',\n",
       " 'someon',\n",
       " 'follow',\n",
       " 'jerki',\n",
       " 'wife',\n",
       " 'acid',\n",
       " 'area',\n",
       " 'glass',\n",
       " 'alreadi',\n",
       " 'carbon',\n",
       " 'insid',\n",
       " 'raw',\n",
       " 'deliveri',\n",
       " 'pet',\n",
       " 'huge',\n",
       " 'interest',\n",
       " 'fantast',\n",
       " 'coat',\n",
       " 'world',\n",
       " 'mint',\n",
       " 'thick',\n",
       " 'bold',\n",
       " 'later',\n",
       " 'sold',\n",
       " 'french',\n",
       " 'pour',\n",
       " 'supermarket',\n",
       " 'idea',\n",
       " 'aftertast',\n",
       " 'mine',\n",
       " 'truli',\n",
       " 'refresh',\n",
       " 'licoric',\n",
       " 'wrong',\n",
       " 'discov',\n",
       " 'chili',\n",
       " 'watch',\n",
       " 'toy',\n",
       " 'roll',\n",
       " 'throw',\n",
       " 'cover',\n",
       " 'import',\n",
       " 'beverag',\n",
       " 'garlic',\n",
       " 'dinner',\n",
       " 'larger',\n",
       " 'boil',\n",
       " 'true',\n",
       " 'due',\n",
       " 'hold',\n",
       " 'creami',\n",
       " 'share',\n",
       " 'sourc',\n",
       " 'liquid',\n",
       " 'consum',\n",
       " 'ate',\n",
       " 'saw',\n",
       " 'hit',\n",
       " 'togeth',\n",
       " 'allergi',\n",
       " 'rest',\n",
       " 'caus',\n",
       " 'wast',\n",
       " 'hint',\n",
       " 'dress',\n",
       " 'preserv',\n",
       " 'maker',\n",
       " 'restaur',\n",
       " 'yes',\n",
       " 'issu',\n",
       " 'general',\n",
       " 'miss',\n",
       " 'mind',\n",
       " 'balanc',\n",
       " 'five',\n",
       " 'produc',\n",
       " 'clear',\n",
       " 'break',\n",
       " 'gone',\n",
       " 'slice',\n",
       " 'sour',\n",
       " 'sale',\n",
       " 'herb',\n",
       " 'difficult',\n",
       " 'sent',\n",
       " 'fun',\n",
       " 'told',\n",
       " 'suppos',\n",
       " 'lower',\n",
       " 'melt',\n",
       " 'mention',\n",
       " 'warm',\n",
       " 'loos',\n",
       " 'simpl',\n",
       " 'done',\n",
       " 'within',\n",
       " 'drop',\n",
       " 'return',\n",
       " 'worri',\n",
       " 'stomach',\n",
       " 'crunch',\n",
       " 'unfortun',\n",
       " 'veggi',\n",
       " 'awesom',\n",
       " 'plant',\n",
       " 'option',\n",
       " 'dip',\n",
       " 'bottom',\n",
       " 'beat',\n",
       " 'anyway',\n",
       " 'wrap',\n",
       " 'excit',\n",
       " 'crave',\n",
       " 'amazoncom',\n",
       " 'impress',\n",
       " 'healthier',\n",
       " 'tradit',\n",
       " 'plan',\n",
       " 'thin',\n",
       " 'batch',\n",
       " 'starbuck',\n",
       " 'easier',\n",
       " 'allow',\n",
       " 'picki',\n",
       " 'seen',\n",
       " 'unlik',\n",
       " 'onion',\n",
       " 'level',\n",
       " 'anywher',\n",
       " 'shape',\n",
       " 'bone',\n",
       " 'under',\n",
       " 'vet',\n",
       " 'raisin',\n",
       " 'twice',\n",
       " 'banana',\n",
       " 'grow',\n",
       " 'fit',\n",
       " 'figur',\n",
       " 'drinker',\n",
       " 'immedi',\n",
       " 'avoid',\n",
       " 'fri',\n",
       " 'beauti',\n",
       " 'soon',\n",
       " 'yogurt',\n",
       " 'stir',\n",
       " 'happen',\n",
       " 'busi',\n",
       " 'test',\n",
       " 'grey',\n",
       " 'opinion',\n",
       " 'kitchen',\n",
       " 'diabet',\n",
       " 'yourself',\n",
       " 'possibl',\n",
       " 'tuna',\n",
       " 'toast',\n",
       " 'italian',\n",
       " 'websit',\n",
       " 'show',\n",
       " 'crisp',\n",
       " 'unless',\n",
       " 'number',\n",
       " 'sort',\n",
       " 'none',\n",
       " 'chemic',\n",
       " 'tend',\n",
       " 'blueberri',\n",
       " 'cool',\n",
       " 'form',\n",
       " 'requir',\n",
       " 'seal',\n",
       " 'hate',\n",
       " 'moist',\n",
       " 'sandwich',\n",
       " 'refriger',\n",
       " 'children',\n",
       " 'remind',\n",
       " 'skin',\n",
       " 'concern',\n",
       " 'do',\n",
       " 'vinegar',\n",
       " 'send',\n",
       " 'sampl',\n",
       " 'uniqu',\n",
       " 'digest',\n",
       " 'six',\n",
       " 'matter',\n",
       " 'browni',\n",
       " 'homemad',\n",
       " 'shake',\n",
       " 'today',\n",
       " 'perhap',\n",
       " 'pie',\n",
       " 'stand',\n",
       " 'tin',\n",
       " 'cheap',\n",
       " 'mapl',\n",
       " 'site',\n",
       " 'basic',\n",
       " 'medium',\n",
       " 'manufactur',\n",
       " 'agre',\n",
       " 'wine',\n",
       " 'blue',\n",
       " 'describ',\n",
       " 'subscrib',\n",
       " 'short',\n",
       " 'trip',\n",
       " 'blood',\n",
       " 'earl',\n",
       " 'touch',\n",
       " 'condit',\n",
       " 'suppli',\n",
       " 'flower',\n",
       " 'keurig',\n",
       " 'lover',\n",
       " 'main',\n",
       " 'shipment',\n",
       " 'tini',\n",
       " 'extract',\n",
       " 'trap',\n",
       " 'standard',\n",
       " 'sprinkl',\n",
       " 'typic',\n",
       " 'previous',\n",
       " 'stevia',\n",
       " 'biscuit',\n",
       " 'gourmet',\n",
       " 'steep',\n",
       " 'tart',\n",
       " 'appreci',\n",
       " 'splenda',\n",
       " 'expir',\n",
       " 'hook',\n",
       " 'broken',\n",
       " 'improv',\n",
       " 'realiz',\n",
       " 'muffin',\n",
       " 'readi',\n",
       " 'american',\n",
       " 'bland',\n",
       " 'concentr',\n",
       " 'instruct',\n",
       " 'complaint',\n",
       " 'travel',\n",
       " 'mill',\n",
       " 'anymor',\n",
       " 'move',\n",
       " 'imagin',\n",
       " 'overpow',\n",
       " 'raspberri',\n",
       " 'tree',\n",
       " 'bear',\n",
       " 'higher',\n",
       " 'remov',\n",
       " 'word',\n",
       " 'somewhat',\n",
       " 'research',\n",
       " 'quantiti',\n",
       " 'portion',\n",
       " 'reduc',\n",
       " 'visit',\n",
       " 'dessert',\n",
       " 'mom',\n",
       " 'whatev',\n",
       " 'offic',\n",
       " 'incred',\n",
       " 'caramel',\n",
       " 'shelf',\n",
       " 'doubl',\n",
       " 'english',\n",
       " 'count',\n",
       " 'creat',\n",
       " 'pan',\n",
       " 'pocket',\n",
       " 'otherwis',\n",
       " 'seller',\n",
       " 'learn',\n",
       " 'straight',\n",
       " 'sound',\n",
       " 'pizza',\n",
       " 'leaf',\n",
       " 'stale',\n",
       " 'decent',\n",
       " 'frozen',\n",
       " 'subtl',\n",
       " 'broth',\n",
       " 'sea',\n",
       " 'tablespoon',\n",
       " 'heavi',\n",
       " 'thrill',\n",
       " 'spoon',\n",
       " 'grape',\n",
       " 'waffl',\n",
       " 'kept',\n",
       " 'spread',\n",
       " 'claim',\n",
       " 'lose',\n",
       " 'senseo',\n",
       " 'summer',\n",
       " 'cashew',\n",
       " 'pictur',\n",
       " 'weak',\n",
       " 'brought',\n",
       " 'ball',\n",
       " 'forward',\n",
       " 'dollar',\n",
       " 'salmon',\n",
       " 'parti',\n",
       " 'control',\n",
       " 'system',\n",
       " 'chunk',\n",
       " 'train',\n",
       " 'kick',\n",
       " 'mess',\n",
       " 'curri',\n",
       " 'serious',\n",
       " 'flake',\n",
       " 'puppi',\n",
       " 'spend',\n",
       " 'across',\n",
       " 'mustard',\n",
       " 'terribl',\n",
       " 'vegan',\n",
       " 'increas',\n",
       " 'knew',\n",
       " 'felt',\n",
       " 'appear',\n",
       " 'charg',\n",
       " 'inform',\n",
       " 'various',\n",
       " 'pricey',\n",
       " 'chanc',\n",
       " 'chop',\n",
       " 'plenti',\n",
       " 'lack',\n",
       " 'sick',\n",
       " 'hazelnut',\n",
       " 'room',\n",
       " 'outsid',\n",
       " 'stronger',\n",
       " 'teaspoon',\n",
       " 'alon',\n",
       " 'known',\n",
       " 'age',\n",
       " 'afternoon',\n",
       " 'herbal',\n",
       " 'relat',\n",
       " 'fall',\n",
       " 'heart',\n",
       " 'heard',\n",
       " 'freez',\n",
       " 'yeast',\n",
       " 'present',\n",
       " 'play',\n",
       " 'job',\n",
       " 'troubl',\n",
       " 'vegetarian',\n",
       " 'comment',\n",
       " 'paper',\n",
       " 'descript',\n",
       " 'crust',\n",
       " 'celiac',\n",
       " 'power',\n",
       " 'style',\n",
       " 'smoke',\n",
       " 'pouch',\n",
       " 'burn',\n",
       " 'jelli',\n",
       " 'whether',\n",
       " 'grill',\n",
       " 'premium',\n",
       " 'late',\n",
       " 'sensit',\n",
       " 'prompt',\n",
       " 'fridg',\n",
       " 'mother',\n",
       " 'crazi',\n",
       " 'pretzel',\n",
       " 'book',\n",
       " 'school',\n",
       " 'pass',\n",
       " 'crispi',\n",
       " 'everyday',\n",
       " 'paid',\n",
       " 'indian',\n",
       " 'aw',\n",
       " 'understand',\n",
       " 'nutti',\n",
       " 'child',\n",
       " 'grind',\n",
       " 'safe',\n",
       " 'quinoa',\n",
       " 'introduc',\n",
       " 'sticki',\n",
       " 'allerg',\n",
       " 'supplement',\n",
       " 'agav',\n",
       " 'remain',\n",
       " 'equal',\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can rememb see the show when air televis year ago when was child sister later bought the which have this day thirti somethingi use this seri book song when did student teach for preschool turn the whole school now purchas along with the book for children the tradit live'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words= 5000\n",
    "sorted_list= sorted_list[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Sentences of words to sequence of rank number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=[]\n",
    "for sent in Data_x:\n",
    "    lis=[]\n",
    "    for word in sent.split():\n",
    "        if word in sorted_list:\n",
    "            lis.append(word)\n",
    "    column.append(' '.join(lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('column.pkl','wb') as file:\n",
    "    pickle.dump(column,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x=[]\n",
    "for sent in Data_x:\n",
    "    lis=[]\n",
    "    for word in sent.split():\n",
    "        if word in sorted_list:\n",
    "            lis.append(sorted_list.index(word)+1)\n",
    "    final_x.append(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest= final_x[:30000]\n",
    "Ytest= Data_y[:30000]\n",
    "Xtrain= final_x[30000:]\n",
    "Ytrain= Data_y[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 216, 209, 1980, 106, 8, 368, 25, 3960, 528, 3217, 3217, 721, 17, 488, 1101, 2, 583, 127, 5, 1071, 474, 5, 11, 697, 8, 47, 28, 1, 557, 16, 3217, 3217, 9, 322, 1, 31, 89, 99, 12, 10, 156, 622, 42, 91, 151, 69, 541, 35, 1134, 17, 61, 5, 8, 352, 1, 69]\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_review_length=600\n",
    "Xtrain = sequence.pad_sequences(Xtrain, maxlen=max_review_length)\n",
    "Xtest= sequence.pad_sequences(Xtest, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    8  216  209\n",
      " 1980  106    8  368   25 3960  528 3217 3217  721   17  488 1101    2\n",
      "  583  127    5 1071  474    5   11  697    8   47   28    1  557   16\n",
      " 3217 3217    9  322    1   31   89   99   12   10  156  622   42   91\n",
      "  151   69  541   35 1134   17   61    5    8  352    1   69]\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9021134197561777072\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3206820659\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17023240268385776602\n",
      "physical_device_desc: \"device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 same as of IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 600, 32)           160032    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,333\n",
      "Trainable params: 213,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "70000/70000 [==============================] - 1607s 23ms/step - loss: 0.2394 - acc: 0.9088\n",
      "Epoch 2/10\n",
      "70000/70000 [==============================] - 1620s 23ms/step - loss: 0.1763 - acc: 0.9321\n",
      "Epoch 3/10\n",
      "70000/70000 [==============================] - 1715s 24ms/step - loss: 0.1579 - acc: 0.9392\n",
      "Epoch 4/10\n",
      "70000/70000 [==============================] - 1468s 21ms/step - loss: 0.1422 - acc: 0.9458\n",
      "Epoch 5/10\n",
      "70000/70000 [==============================] - 1459s 21ms/step - loss: 0.1260 - acc: 0.9525\n",
      "Epoch 6/10\n",
      "70000/70000 [==============================] - 1518s 22ms/step - loss: 0.1114 - acc: 0.9583\n",
      "Epoch 7/10\n",
      "70000/70000 [==============================] - 1522s 22ms/step - loss: 0.0993 - acc: 0.9631\n",
      "Epoch 8/10\n",
      "70000/70000 [==============================] - 1554s 22ms/step - loss: 0.0873 - acc: 0.9674\n",
      "Epoch 9/10\n",
      "70000/70000 [==============================] - 1459s 21ms/step - loss: 0.0784 - acc: 0.9716\n",
      "Epoch 10/10\n",
      "70000/70000 [==============================] - 1445s 21ms/step - loss: 0.0668 - acc: 0.9756\n",
      "Accuracy: 92.31%\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(Xtrain, Ytrain, nb_epoch=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 600, 32)           160032    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 600, 100)          53200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 600, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 80)                57920     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 271,233\n",
      "Trainable params: 271,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n",
    "model2.add(LSTM(100,return_sequences=True))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(LSTM(80))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "70000/70000 [==============================] - 2961s 42ms/step - loss: 0.2402 - acc: 0.9088\n",
      "Epoch 2/10\n",
      "70000/70000 [==============================] - 2958s 42ms/step - loss: 0.1802 - acc: 0.9316\n",
      "Epoch 3/10\n",
      "70000/70000 [==============================] - 2961s 42ms/step - loss: 0.1613 - acc: 0.9386\n",
      "Epoch 4/10\n",
      "70000/70000 [==============================] - 2958s 42ms/step - loss: 0.1429 - acc: 0.9459\n",
      "Epoch 5/10\n",
      "70000/70000 [==============================] - 2964s 42ms/step - loss: 0.1254 - acc: 0.9528\n",
      "Epoch 6/10\n",
      "70000/70000 [==============================] - 3007s 43ms/step - loss: 0.1108 - acc: 0.9592\n",
      "Epoch 7/10\n",
      "70000/70000 [==============================] - 2996s 43ms/step - loss: 0.0970 - acc: 0.9653\n",
      "Epoch 8/10\n",
      "70000/70000 [==============================] - 3001s 43ms/step - loss: 0.0839 - acc: 0.9706\n",
      "Epoch 9/10\n",
      "70000/70000 [==============================] - 3001s 43ms/step - loss: 0.0764 - acc: 0.9736\n",
      "Epoch 10/10\n",
      "70000/70000 [==============================] - 3011s 43ms/step - loss: 0.0667 - acc: 0.9778\n",
      "Accuracy: 92.96%\n"
     ]
    }
   ],
   "source": [
    "history2= model2.fit(Xtrain, Ytrain, nb_epoch=10, batch_size=64)-# Final evaluation of the model\n",
    "scores = model2.evaluate(Xtest, Ytest, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl','wb') as file:\n",
    "    pickle.dump(model,file)\n",
    "with open('model2.pkl','wb') as file:\n",
    "    pickle.dump(model2,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model on self made review sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a function which convert sentance to requred vectorized format that will feed well in model\n",
    "\n",
    "def cleanhtml(sentance): #substitute expression contained in <> with ' '\n",
    "    cleaned= re.sub(re.compile('<.*?>'),' ',sentance)\n",
    "    return cleaned\n",
    "#function for removing punctuations chars\n",
    "def cleanpunc(sentance):\n",
    "    cleaned= re.sub(r'[?|!|\\'|\"|#]',r'',sentance)\n",
    "    cleaned= re.sub(r'[.|,|)|(|\\|/]',r'',sentance)\n",
    "    return cleaned\n",
    "snowstem= sno('english')\n",
    "\n",
    "def predict_this(sentance):\n",
    "    i=0\n",
    "    str1=' '\n",
    "    final_string=[]\n",
    "    all_positive_words=[] # store words from +ve reviews here\n",
    "    all_negative_words=[] # store words from -ve reviews here.\n",
    "    sent= sentance\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        # we have used cleanpunc(w).split(), one more split function here \n",
    "        # because consider w=\"abc.def\", cleanpunc(w) will return \"abc def\"\n",
    "        # if we dont use .split() function then we will be considring \"abc def\" \n",
    "        # as a single word, but if you use .split() function we will get \"abc\", \"def\"\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                s=(snowstem.stem(cleaned_words.lower())).encode('utf8')\n",
    "                filtered_sentence.append(s)\n",
    "                if(data['Score'].values)[i] =='Positive':\n",
    "                    all_positive_words.append(s)\n",
    "                if(data['Score'].values)[i] =='Negative':\n",
    "                    all_negative_words.append(s)\n",
    "            else:\n",
    "                continue\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    final_string.append(str1)\n",
    "\n",
    "    final_string\n",
    "    for i in final_string:\n",
    "        final_string=i.decode(\"utf-8\")\n",
    "\n",
    "    lis=[]\n",
    "    for word in final_string.split():\n",
    "        if word in sorted_list:\n",
    "            lis.append(sorted_list.index(word)+1)\n",
    "\n",
    "    final_string= lis\n",
    "    final_string = sequence.pad_sequences([final_string], maxlen=max_review_length)\n",
    "    print(final_string)\n",
    "    what= ''\n",
    "    if (round(float(model2.predict(final_string)))==1):\n",
    "        what= 'Positive'\n",
    "        acc= round(float(model2.predict(final_string))*100,2)\n",
    "    else:\n",
    "        what='Negative'\n",
    "        acc= 100- round(float(model2.predict(final_string))*100,2)\n",
    "    print(what,'review with',acc,'% Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  46  13  25 240  15]]\n",
      "Negative review with 97.99 % Accuracy\n"
     ]
    }
   ],
   "source": [
    "sentance= 'food was very bad in taste'\n",
    "predict_this(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  15  73  13 615]]\n",
      "Positive review with 98.01 % Accuracy\n"
     ]
    }
   ],
   "source": [
    "sentance= 'taste of chocolate was fantastic'\n",
    "predict_this(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  46  13 800 202]]\n",
      "Positive review with 97.55 % Accuracy\n"
     ]
    }
   ],
   "source": [
    "sentance= 'food was medium tasty'\n",
    "predict_this(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0   24  281  513 3638  189  276   65  200  188  201\n",
      "  2751   11  208    3   13 2452    1 1185 1402 2550    1   24]]\n",
      "Negative review with 71.84 % Accuracy\n"
     ]
    }
   ],
   "source": [
    "print(data['Text'][1])\n",
    "predict_this(data['Text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "* Accuracy of First model is 92.31%\n",
    "* Accuracy of Second model is 92.96%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
